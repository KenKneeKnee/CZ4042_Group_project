{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath('../scripts/scripts/'))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "\n",
    "from scripts.constants import PATH_TO_DATA, DATA_FILE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>3</td>\n",
       "      <td>If you want to see a train wreck that you ca ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7381</th>\n",
       "      <td>2</td>\n",
       "      <td>Like most of Jaglom 's films , some of it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6627</th>\n",
       "      <td>2</td>\n",
       "      <td>Neither as scary-funny as Tremors nor demente...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                             review\n",
       "636           3   If you want to see a train wreck that you ca ...\n",
       "7381          2   Like most of Jaglom 's films , some of it is ...\n",
       "6627          2   Neither as scary-funny as Tremors nor demente..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(PATH_TO_DATA, DATA_FILE_NAME), sep='|', names=['sentiment', 'review'], encoding='latin-1')\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8544 entries, 0 to 8543\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  8544 non-null   int64 \n",
      " 1   review     8544 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 133.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 3, 2, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "4    2322\n",
       "2    2218\n",
       "3    1624\n",
       "5    1288\n",
       "1    1092\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7314</th>\n",
       "      <td>2</td>\n",
       "      <td>TV skit-com material fervently deposited on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>1</td>\n",
       "      <td>That such a horrible movie could have sprung ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>3</td>\n",
       "      <td>It 's Young Guns meets Goodfellas in this eas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                             review\n",
       "7314          2   TV skit-com material fervently deposited on t...\n",
       "7335          1   That such a horrible movie could have sprung ...\n",
       "5551          3   It 's Young Guns meets Goodfellas in this eas..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda \n",
    "The purpose of this notebook is \"To compare the performance of different Transformers architectures\".\n",
    "\n",
    "<b>General Approach</b>\n",
    "1. Research on the different types of Transformers architectures.\n",
    "2. Conduct analysis on the performance of different Transformers architectures on the given dataset.\n",
    "3. Report the analysis of performance.\n",
    "\n",
    "\n",
    "<b>References</b>\n",
    "1. https://machinelearningmastery.com/the-transformer-model/\n",
    "2. https://huggingface.co/docs/transformers/index\n",
    "3. https://pytorch.org/hub/huggingface_pytorch-transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reseach on the different types of Transformers Architectures\n",
    "\n",
    "There are multiple different types of Transformers Architectures available in the open source market now. With a myriad of options to choose from, one can only decide which one to use based on the performance of the Transformers architecture on the given dataset. In this notebook, I will be exploring the use of Transformers (and their pretrained model weights) to perform sentiment analysis on the given dataset.\n",
    "\n",
    "I will be using the `pytorch-transformers` package to implement the models. In this package, there are a total of 8 pretrained models to choose from.\n",
    "- BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n",
    "- GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n",
    "- GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\n",
    "- Transformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n",
    "- XLNet (from Google/CMU) released with the paper ​XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n",
    "- XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.\n",
    "- RoBERTa (from Facebook), released together with the paper a Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n",
    "- DistilBERT (from HuggingFace), released together with the blogpost Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT by Victor Sanh, Lysandre Debut and Thomas Wolf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing BERT for Sentiment Analysis\n",
    "Reference: https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb and https://huggingface.co/docs/transformers/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peanutsee/Desktop/Year 4 Sem 1/CZ4042 - Neural Network & Deep Learning/Group Project/pyenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from string import punctuation\n",
    "\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "HfFolder.save_token('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define BERT Model\n",
    "bert = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# Initialise tokenizer for BERT uncased\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='n_tokens'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the maximum number of possible tokens\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Define pre-processing function (remove numbers and punctuations)\n",
    "def preprocess_text(text:str) -> str:\n",
    "    # Case text to lower case\n",
    "    text_lowercase = text.lower()\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    text_no_ws = \" \".join([x for x in text_lowercase.split(\" \") if x != \" \"])\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text_no_punc = ''.join([x for x in text_no_ws if x not in punctuation])\n",
    "    \n",
    "    # Remove numbers\n",
    "    text_no_num = ''.join([x for x in text_no_punc if x.isalpha() or x == \" \"])\n",
    "    \n",
    "    return text_no_num.strip()\n",
    "\n",
    "df_copy['review_cleaned'] = df_copy['review'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Count number of tokens\n",
    "df_copy['n_tokens'] = df_copy['review_cleaned'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# Display counts for tokens\n",
    "sns.boxplot(data=df_copy, x='n_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 58\n",
      "Min: 1\n"
     ]
    }
   ],
   "source": [
    "n_tokens_max, n_tokens_min = df_copy['n_tokens'].max(), df_copy['n_tokens'].min()\n",
    "print(f'Max: {n_tokens_max}\\nMin: {n_tokens_min}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(map(preprocess_text, df['review']))\n",
    "labels = list(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for review, label in zip(reviews, labels):\n",
    "    dataset.append({\"text\": review, \"label\": str(label)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return {\n",
    "        \"text\": examples['text'],\n",
    "        \"label\": int(examples['label']),\n",
    "        **tokenizer(examples[\"text\"], truncation=True)\n",
    "    }\n",
    "\n",
    "tokenized_train= list(map(preprocess_function, train))\n",
    "tokenized_test= list(map(preprocess_function, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 0/505 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                                 \n",
      " 20%|██        | 101/505 [01:02<04:26,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9147855639457703, 'eval_accuracy': 0.43913857677902624, 'eval_runtime': 5.5543, 'eval_samples_per_second': 384.568, 'eval_steps_per_second': 6.121, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|████      | 202/505 [02:06<02:48,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.870308518409729, 'eval_accuracy': 0.4559925093632959, 'eval_runtime': 5.2155, 'eval_samples_per_second': 409.547, 'eval_steps_per_second': 6.519, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 303/505 [03:09<01:29,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8513351678848267, 'eval_accuracy': 0.47659176029962547, 'eval_runtime': 5.1448, 'eval_samples_per_second': 415.173, 'eval_steps_per_second': 6.609, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 404/505 [04:13<00:42,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.902057409286499, 'eval_accuracy': 0.4789325842696629, 'eval_runtime': 5.6416, 'eval_samples_per_second': 378.613, 'eval_steps_per_second': 6.027, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 500/505 [05:16<00:02,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.752, 'learning_rate': 1.9801980198019803e-07, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 505/505 [05:26<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9106379151344299, 'eval_accuracy': 0.4798689138576779, 'eval_runtime': 7.0588, 'eval_samples_per_second': 302.6, 'eval_steps_per_second': 4.817, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 505/505 [05:29<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 329.2955, 'train_samples_per_second': 97.299, 'train_steps_per_second': 1.534, 'train_loss': 0.7494500264082805, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=505, training_loss=0.7494500264082805, metrics={'train_runtime': 329.2955, 'train_samples_per_second': 97.299, 'train_steps_per_second': 1.534, 'train_loss': 0.7494500264082805, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_uncased_trained\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_4', 'score': 0.9299127459526062}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'my kukubird is very big!! omg!! I love it!!'\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"./bert_uncased_trained/\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
