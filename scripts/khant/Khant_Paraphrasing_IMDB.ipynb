{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85bedcf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khant\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "from parrot import Parrot\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bf3920d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5ea09b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2744d67d4f0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4042)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d33cf",
   "metadata": {},
   "source": [
    "# IMDB data set exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd54cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0a2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean reviews\n",
    "imdb['review'] = imdb['review'].apply(lambda x: re.sub(r'<br\\s*/?>', '', x, flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e486d",
   "metadata": {},
   "source": [
    "# Exploring Paraphrasing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ecfe2869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Humarin's paraphraser based on chatgpt\n",
    "\n",
    "tokenizer_p = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "n = 4 # Generate n paraphrases\n",
    "\n",
    "def paraphrase(\n",
    "    question,\n",
    "    num_beams= n,\n",
    "    num_beam_groups= n,\n",
    "    num_return_sequences= n,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=128\n",
    "):\n",
    "    input_ids = tokenizer_p(\n",
    "        f'paraphrase: {question}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    res = tokenizer_p.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d039593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khant\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\khant\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "C:\\Users\\khant\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prithivida's python paraphrase\n",
    "\n",
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2879cafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parrot output:\n",
      "this model doesn't meet my expectations\n",
      "this model doesn't deliver my expectations\n",
      "this is not meeting my expectations\n",
      "this model does not perform to my expectations\n",
      "--------------------------------------------------------\n",
      "ChatGPT paraphraser output:\n",
      "This model is not meeting my expectations.\n",
      "I am not satisfied with the performance of this model.\n",
      "The quality of this model is not satisfactory.\n",
      "My impressions of this model are not up to par.\n"
     ]
    }
   ],
   "source": [
    "# Testing our paraphraser\n",
    "\n",
    "phrase = \"this model is not performing up to my expectations\"\n",
    "\n",
    "print(\"Parrot output:\")\n",
    "para_phrases = parrot.augment(input_phrase=phrase, use_gpu=False, max_return_phrases = 4) # returns (string, len(string))\n",
    "for para_phrase in para_phrases:\n",
    "    print(para_phrase[0])\n",
    "    \n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "print(\"ChatGPT paraphraser output:\")\n",
    "para_phrases2 = paraphrase(phrase)\n",
    "for para_phrase in para_phrases2:\n",
    "    print(para_phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e239478",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parrot output:\n",
      "a welcome relief from baseball films that try too hard to be mythic this is a sweet and modest and ultimately winning story\n",
      "a welcome relief from baseball films that try too hard to be mythic this one is a sweet and modest and ultimately winning story\n",
      "a welcome relief from baseball movies that try too hard to be mythic this one is a sweet and modest and ultimately winning story\n",
      "--------------------------------------------------------\n",
      "Humarin's paraphraser output:\n",
      "Unlike baseball movies that strive to be overhyped, this story is both humble and ultimately successful.\n",
      "This baseball movie is a welcome change from the overly ambitious and overblown tale of triumphant team members, as it's genuinely sweet and modest.\n",
      "It's a welcome change from baseball movies that strive to be mythical, as it'll end up being genuinely sweet, modest, and ultimately successful.\n"
     ]
    }
   ],
   "source": [
    "phrase = \" A welcome relief from baseball movies that try too hard to be mythic , this one is a sweet and modest and ultimately winning story .\"\n",
    "\n",
    "print(\"Parrot output:\")\n",
    "para_phrases = parrot.augment(input_phrase=phrase, use_gpu=False,max_length=len(phrase)) # returns (string, len(string))\n",
    "for para_phrase in para_phrases:\n",
    "    print(para_phrase[0])\n",
    "    \n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "print(\"Humarin's paraphraser output:\")\n",
    "para_phrases2 = paraphrase(phrase)\n",
    "for para_phrase in para_phrases2:\n",
    "    print(para_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d49e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = imdb[\"review\"].values[random.randint(0,50000)]\n",
    "print(f'review: {phrase}')\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "print(\"Parrot output:\")\n",
    "para_phrases = parrot.augment(input_phrase=phrase, use_gpu=False) # returns (string, len(string))\n",
    "for para_phrase in para_phrases:\n",
    "    print(para_phrase[0])\n",
    "    \n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "print(\"Humarin's paraphraser output:\")\n",
    "para_phrases2 = paraphrase(phrase)\n",
    "for para_phrase in para_phrases2:\n",
    "    print(para_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0163cb9",
   "metadata": {},
   "source": [
    "*Parrot does not work for multi sentence strings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d59b1e",
   "metadata": {},
   "source": [
    "# Validating paraphrasing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e57cfe",
   "metadata": {},
   "source": [
    "### Sentence wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895d7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Nltk.vader's Sentiment intensity Analyser\n",
    "sia = SIA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78bf184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base score: 0.9217\n",
      "--------------------------------------------------------\n",
      "Humarin's paraphraser output:\n",
      "para score: 0.8074\n",
      "difference: 0.11429999999999996\n",
      "para score: 0.9081\n",
      "difference: 0.013599999999999945\n",
      "para score: 0.8074\n",
      "difference: 0.11429999999999996\n"
     ]
    }
   ],
   "source": [
    "phrase = 'This is an outstanding movie with a great cast. The plot is equally great'\n",
    "base_score = sia.polarity_scores(phrase)['compound']\n",
    "print(f'base score: {base_score}')\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "print(\"Humarin's paraphraser output:\")\n",
    "para_phrases2 = paraphrase(phrase)\n",
    "for para_phrase in para_phrases2:\n",
    "    para_score = sia.polarity_scores(para_phrase)['compound']\n",
    "    print(f'para score: {para_score}')\n",
    "    print(f'difference: {abs(para_score - base_score)}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9ebae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base score: -0.9568\n",
      "--------------------------------------------------------\n",
      "Parrot output:\n",
      "para score: -0.9568\n",
      "difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "phrase = imdb[\"review\"].values[random.randint(0,50000)]\n",
    "base_score = sia.polarity_scores(phrase)['compound']\n",
    "print(f'base score: {base_score}')\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "print(\"Parrot output:\")\n",
    "para_phrases = parrot.augment(input_phrase=phrase, use_gpu=False) # returns (string, len(string))\n",
    "for para_phrase in para_phrases:\n",
    "    para_score = sia.polarity_scores(para_phrase[0])['compound']\n",
    "    print(f'para score: {para_score}')\n",
    "    print(f'difference: {abs(para_score - base_score)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05ac1157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base score: 0.9642\n",
      "--------------------------------------------------------\n",
      "Humarin's paraphraser output:\n",
      "para score: 0.9136\n",
      "difference: 0.05059999999999998\n",
      "para score: 0.9325\n",
      "difference: 0.03169999999999995\n",
      "para score: 0.9698\n",
      "difference: 0.005600000000000049\n"
     ]
    }
   ],
   "source": [
    "phrase = imdb[\"review\"].values[random.randint(0,50000)]\n",
    "base_score = sia.polarity_scores(phrase)['compound']\n",
    "print(f'base score: {base_score}')\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "print(\"Humarin's paraphraser output:\")\n",
    "para_phrases2 = paraphrase(phrase)\n",
    "for para_phrase in para_phrases2:\n",
    "    para_score = sia.polarity_scores(para_phrase)['compound']\n",
    "    print(f'para score: {para_score}')\n",
    "    print(f'difference: {abs(para_score - base_score)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06898e5b",
   "metadata": {},
   "source": [
    "Sentence wise paraphrasing works better for parrot, if sentence is actually paraphrased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667ecb3",
   "metadata": {},
   "source": [
    "# Sentencewise paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90afca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am totally addicted to this show. i can't wait till the week goes by to see the next showing. it's a great story line and it has the best actors and actresses on the show. i will tune in every week to watch it even if i am not home i always have my vcr set to tape monarch cove. simon rex is the best actor on the show. it is suspenseful and exciting. i think this show should stay on the air and i believe everyone should tune in to watch it. i saw the very first episode and actually i wasn't going to watch it but i was watching lifetime one day and i decided to watch it because it was on and i absolutely love it and right now it's my favorite show. i am really mean it.\n"
     ]
    }
   ],
   "source": [
    "print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21524bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am totally addicted to this show\n",
      " i can't wait till the week goes by to see the next showing\n",
      " it's a great story line and it has the best actors and actresses on the show\n",
      " i will tune in every week to watch it even if i am not home i always have my vcr set to tape monarch cove\n",
      " simon rex is the best actor on the show\n",
      " it is suspenseful and exciting\n",
      " i think this show should stay on the air and i believe everyone should tune in to watch it\n",
      " i saw the very first episode and actually i wasn't going to watch it but i was watching lifetime one day and i decided to watch it because it was on and i absolutely love it and right now it's my favorite show\n",
      " i am really mean it\n",
      "\n",
      "para score: 0.9648\n",
      "difference: 0.0006000000000000449\n",
      "para score: 0.9611\n",
      "difference: 0.0030999999999999917\n",
      "para score: 0.9593\n",
      "difference: 0.004899999999999904\n"
     ]
    }
   ],
   "source": [
    "sentences = phrase.split(\".\")\n",
    "\n",
    "res = ['','','']\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    para_phrase =paraphrase(s)\n",
    "    for i in range(len(res)):\n",
    "        res[i] += para_phrase[i]\n",
    "\n",
    "for rs in res:\n",
    "    para_score = sia.polarity_scores(rs)['compound']\n",
    "    print(f'para score: {para_score}')\n",
    "    print(f'difference: {abs(para_score - base_score)}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75a74263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am completely fixated on this show and it is my obsession.The next week is approaching and I'm eagerly anticipating the screening.The storyline is impressive and the actors and actresses on screen are outstanding.Even if I am not at home, I will watch it every week and have my VCR taped to record Monarch Cove.The most outstanding actor on the show is Simon Rex.The story is teeming with suspense and excitement.I am of the opinion that this show should continue to air and be viewed by all.I had no intention of watching the first episode, but instead, I watched Lifetime because it was shown and I loved it. It's my favorite show at the moment.I am genuinely unpleasant.What is the definition of \"I don't want to be a robot,\" and what does it mean when someone says something like, \"If I wanted to make sure that some of their actions were not harmful\", etc.?\n",
      "\n",
      "This show has become my absolute obsession.I'm eagerly anticipating the upcoming screening until the week following our show.The show's storyline is impressive, and its cast includes the most talented actors and actresses.I will watch it every week even when I am not at home, and my VCR is always set to taped at Monarch Cove.Siddie Rex is the most outstanding actor on the show.It's full of excitement and suspense.My opinion is that this show should continue to be broadcasted and that it should be viewed by all.After watching the first episode, I decided to watch it because of its quality and my love for the show.It is my fault that I am being so cruel.The\n",
      "\n",
      "The show is so captivating that I can't resist giving it a chance.It will be a long time coming for the next screening in just one week.With the most talented actors and actresses on the team, this is a great storyline to follow.Whenever I am not at home, my VCR is always set to tape Monarch Cove every week, and I will continue to watch it even when I'm away from home.The best performer on the show is Sidney Rex.This is a thrilling and captivating story.In my opinion, this program must go on air and everyone should catch it.Despite not intending to watch the first episode, I ended up watching it because of its appearance and my love for the show.My behavior is genuinely unpleasant.Is it possible to have a \"person\" with the same name?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rs in res:\n",
    "    print(rs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecf1cf",
   "metadata": {},
   "source": [
    "Sentecewise paraphrasing produces better score but semantics will be lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001ff9b",
   "metadata": {},
   "source": [
    "# Training using small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "62c2871a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    109\n",
       "negative     91\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_db = imdb.sample(n = 200)\n",
    "small_db['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9569833c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1389.355"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for r in small_db['review'].values:\n",
    "    l.append(len(r))\n",
    "np.mean(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a079b23",
   "metadata": {},
   "source": [
    "## Sentence-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2f586",
   "metadata": {},
   "source": [
    "Takes a really long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ad7bf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The real life case of an innocent First Nations chief(the Indian) by an Winnipeg city officer(the Cowboy) is the basis of this TV movie\n",
      " The actual case caused its fair share of racial tension in Canada, a small scale Martin Luther King thing\n",
      " The misjustice of First Nations people is becoming a staple in the Canadian cinema diet\n",
      " What makes this film worth viewing is the focus on the family's reactions\n",
      " The father played by Gordon Tootoosis demands forgiveness and the brother played by Eric Schweig demands justice\n",
      " The stars Gordon Tootoosis and Adam Beach(WINDTALKERS, SKINWALKERS)have minor, almost cameo, appearances\n",
      " Soon-to-be star Eric Schweig makes his mark in this film with a powerful performance\n",
      " An honourable mention goes to veteran actor Gary Chalk who has chalked up over 100 movies to his credit\n",
      " His portrayal of the troubled soul Inspector Dowson was worthy of a Gemini Award(the Canadian Emmy)along with Eric Schweig\n",
      " The special effects(jump cuts, dream sequences) are occasional and not overbearing\n",
      " Couple this with some beautiful northern Canadian scenery and recent ongoing events involving police officers and First Nations people like the Neil Stonechild case, and you have a very rewarding and relevant viewing experience\n",
      "\n",
      "difference: 0.02639999999999998\n",
      "difference: 0.024499999999999966\n",
      "difference: 0.1825\n",
      "Review 1 done.\n",
      "I was privileged to have seen some snippets from Aardman's original run of this show in the UK\n",
      " It was always fun and always funny\n",
      " None of the charm has been lost in translation--it's as fresh as the people interviewed--whether some of it is scripted or not, as has been rumored, is beside the point\n",
      " It's always entertaining\n",
      "Aardman Animations shows great imagination in the characters used for each voice, the single aspect that I probably love most about the show and its concept (the hostility between the pandas, the porcupines discussing fear of needles, the painting ape)\n",
      " Regulars really grow on you as well, such as the horse and donkey teens from Maryland, most every married couple (the parrots, the insects, and the cats to name a few) and child-voiced character, and I've really come to dig the ferret! Monday's finally become a day to which to look forward\n",
      "\n",
      "difference: 0.053100000000000036\n",
      "difference: 0.061900000000000066\n",
      "difference: 0.05470000000000008\n",
      "Review 2 done.\n",
      "Caught this movie on the tube on a Sunday\n",
      " I thought it was so bad I looked it up on IMDb to see what others thought of it\n",
      " I was not surprised at the amount of silly people who enjoyed this fluff\n",
      " I was however surprised when I looked into the comments to read the Hated It categories only to find that their were none\n",
      " I was shocked at this; I always look at the hated it's as their are always those who hate a movie no matter how good it is\n",
      " Somehow this movie made it through unscathed by the haters I say nay to that and proclaim proudly that I HATE THIS MOVIE! I know I should go into detail about why I hate this movie but to do so would only grant this movie more respect than it deserves\n",
      "\n",
      "difference: 0.5095999999999999\n",
      "difference: 0.1531\n",
      "difference: 0.20589999999999997\n",
      "Review 3 done.\n",
      "I provided location services on the this film every Sunday we would shoot in London's Berkeley Square\n",
      " David Niven ever the gentleman thoroughly enjoyed the role, sadly to be his last\n",
      " we had a moment of panic when a trunk load of fake Krugerrands (cast for the film\n",
      "\n",
      ") tipped down a storm drain\n",
      " Imagine frantic crew opening all the drains to recover every last one\n",
      " If you know and love London you'll love this comedy romp - also starts Richard Jordan who sadly died from a brain tumour\n",
      " A good film, great crew ,superb cast\n",
      " look for the current stars of coronation street then playing crowd scenes or extras\n",
      "The car lot and Ivan's retail enterprises were all shot in west London, Chiswick the entire shopping parade and the American used car lot were dressed overnight, the car lot is still there as are the shops\n",
      " A restaurant was suddenly turned into a funeral parlour\n",
      " If you see the film on the listings make an effort to see it! By the way Sally Harrison the Bank receptionist was married to the production designer Tony Curtis\n",
      "\n",
      "April 2007 Just thought I would add a few extra comments on locations:Pub: just off Berkeley Square Elke Sommers Cottage: in back Road alongide Twickenham Film Studios Ivans Used Car Lot: along Chiswick High Street and all shop locations near roundabout\n",
      " Workshops (converting armoured vans)Factory on roundabout opposite Fullers Brewery Jail (see workshops above) Telephone box see Elke Sommers cottage ( it was the wooden studio prop box used in many films, look for the lighting cable at gound level and the wood hinges on the door!!! Computer room Honeywells near Olympia Graveyard - Chiswick - Grave just outside the boundary on common land Bank interiors, ceiling void and strongroom :Twickenham studiosAnd just to add David Niven ever the gentleman, joked and mixed with the crew, extras and so on\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Niven would dine in the Connaught hotel bu join the crew for coffee!\n",
      "difference: 0.18719999999999992\n",
      "difference: 0.1651999999999999\n",
      "difference: 0.1048\n",
      "Review 4 done.\n",
      "Unhinged was part of the Video Nasty censorship film selection that the UK built up in the 80's\n",
      " Keeps the gory stuff out of the hands of children, don't you know! It must have left many wondering what the fuss was all about\n",
      " By today's standard, Unhinged is a tame little fairy tale\n",
      "3 girls are off to a jazz concert\n",
      "\n",
      "\n",
      " and right away, you know the body count is going to be quite low\n",
      " They get lost in the woods, & wind up getting in a car accident that looks so fake it's laughable\n",
      " They are picked up by some nearby residents that live in the woods in a creepy house\n",
      " One of the girls is seriously injured and has to stay upstairs\n",
      " Then there's talking\n",
      " Talking about why the girls are here, and how they must be to dinner on time because mother doesn't like it when someone is late\n",
      " And more talking\n",
      " Yakkity yak\n",
      " Some suspense is built as a crazy guy is walking around and harassing the girls, and someone's eyeball is looking through holes in the walls at the pretty girls in something that looks like Hitchcock's Psycho\n",
      " I digress because there is so much blah blah in this film, that you wonder when the killings are going to start\n",
      " In fact, one of the girls gets so bored out of her mind that she walks in the woods, alone, looking for the town\n",
      " Smart move\n",
      " She probably knew about the lonely virgin walking alone in the woods part, but just didn't care\n",
      " More talk continues after this as we wait, wait, and wait some more until the next girl may or may not be killed\n",
      "And then there's the twist ending\n",
      " The \"expected\" unexpected for some viewers, for others a real gotcha\n",
      " Quite possibly the ONLY reason why someone would really want to watch this\n",
      " I don't care how twisted it is, nothing in this movie makes up for the most boring time I had watching it\n",
      " Even with the minor impact of the ending, the director just didn't have what it takes to really deliver a good story with it\n",
      " It would have made a much better 30 minute - 1 hour TV episode on say, Tales from the Darkside\n",
      "If you really must get this for any reason, perhaps just to say you've watched every slasher movie, do yourself a favor and have the fast-forward button ready\n",
      " Since the movie has so many unimportant scenes, just zoom through them, and in no time, you'll get to the \"WOW, that's what it was all this time\" ending\n",
      " Oh and halfway through the movie there's a shower scene with 2 girls showing boo-bees\n",
      " Horray for boo-bees\n",
      " Those beautiful buzzing honey-making boo-bees\n",
      "\n",
      "difference: 1.8749\n",
      "difference: 1.5529000000000002\n",
      "difference: 1.8628\n",
      "Review 5 done.\n",
      "This series just gets worse and worse\n",
      " Poorly written and just plain not funny! The premise is excellent, but the writer's inexperience shines through\n",
      " By trying so hard to offend no one they end up insulting everyone\n",
      " Now into the second season the desperate cast have stopped waving their arms about, and resorted to that patronizing, smug, \"Oh, silly you\" style acting that comes with a no laugh script\n",
      " They roll their eyes and shake their heads at each other as if to say, aren't we zany? Isn't this funny? Well, no, it's not actually\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gum disease is less painful\n",
      " No wonder, with the exception of Corner Gas, Canadians generally avoid Canadian TV\n",
      " Come on CBC you're suppose to be our leading station showcasing the best of Canadian talent\n",
      " Pull the plug on this amateurish mess\n",
      "\n",
      "difference: 0.042099999999999915\n",
      "difference: 0.0042000000000000925\n",
      "difference: 0.033399999999999985\n",
      "Review 6 done.\n",
      "I wish I could give this movie a zero, or even lower, because sadly that's what it deserves\n",
      " I honestly never walk out of a movie, but this one was so dreadfully awful that I couldn't stand another minute of it\n",
      " Please,please, please- for the sake of mankind- skip this movie\n",
      " If you want a hot lesbian movie that you can really delve into, this isn't it\n",
      "It has unattractive, unappealing leads, choppy structure, ridiculous dialog, and it is absolutely unconvincing in every imaginable way\n",
      " On an absolutely basic level, it fails to entertain\n",
      " Everything about \"Mango Kiss\" is so stagey, it is WORSE than any student film I have seen\n",
      "As if that weren't enough, the entire movie relies on constant (and I mean CONSTANT) voice over narration\n",
      " The script writer may as well have written a novel, because they insisted on TELLING everything, instead of SHOWING it\n",
      " We are just supposed to assume that Lou and Sassafras are the best of friends and have a special connection, even though there is no character development to illustrate this\n",
      " Also, this film continues to introduce to new characters after the first five minutes, and not in a natural way, but in a freeze-frame of the characters with their name written over the screen\n",
      " There is no introduction, no development of any of the characters\n",
      " We don't really get to know any of them\n",
      "This is the most amateur movie I have ever seen\n",
      "I am a gay woman, who watched this film with my roommate (another gay woman) and we both found this movie to be a depressing representation of queer cinema\n",
      " I am beginning to think that lesbian and gay movies are a lot like Christian rock - it doesn't matter how atrocious it is, people will still flock to it, and champion it, no matter how poor the quality is\n",
      " Please don't rent this!! Instead, let's encourage the production of QUALITY gay and lesbian movies by renting \"But I'm a Cheerleader\" or \"Fire\" or \"Heavenly Creatures\", instead of swallowing whatever mindless tripe they aim at the gay community (i\n",
      "e\n",
      " Queer as Folk)\n",
      "\n",
      "difference: 0.10699999999999998\n",
      "difference: 1.4586999999999999\n",
      "difference: 1.2187000000000001\n",
      "Review 7 done.\n",
      "White Fire has so much going for it\n",
      " With Larry Bird look-alike Robert Ginty leading the charge blazing away with his fabulous hair and super macho mustache, the movie soars above other low-budget actioners\n",
      " The charisma he has in this makes Tom Selleck look like a putz\n",
      " With Ginty beating up everyone, the movie only rises in awesomeness when a story of diamond intrigue enters into play\n",
      " Then add in Fred Williamson, some frontal bush, chainsaw attacks and some awesome incest themes\n",
      "\n",
      "\n",
      "\n",
      "this flick ends up delivering on all cylinders\n",
      " If you're looking for some awesome B-Action, this is where it's at\n",
      " Now, if I can just get my hands on that soundtrack\n",
      "\n",
      "difference: 0.0048000000000000265\n",
      "difference: 0.007699999999999929\n",
      "difference: 0.2942\n",
      "Review 8 done.\n",
      "Worst movie ever made!!! Please see the Real movie reviews from the pros on this movie\n",
      "Check Rotten Tomatoes on the web for some good independent reviews on this film\n",
      " The comments made on this site are apparently from folks with some financial interest in this film\n",
      " I find the positive comments very misleading\n",
      " I find it amazing how the negative comments are so bad against this movie and the positive comments sound like an Academy Awards Speech\n",
      " Don't waste your hard earned money!!!!!! This Film is retarded!! I can't believe a film like this would ever be made\n",
      " Why would Hollywood waste their time on such junk? This film is an attempt at nothing\n",
      " I ask myself what looser would actually sink their money producing such trash\n",
      " I went to blockbuster and the attendant even told us not to waste our time or money\n",
      " I didn't listen and I did waste my time and cash\n",
      " Please don't make the same mistake! It really is the \"Worst movie ever made!\"\n",
      "difference: 0.38470000000000004\n",
      "difference: 1.8340999999999998\n",
      "difference: 0.6163000000000001\n",
      "Review 9 done.\n",
      "One can deal with historical inaccuracies, but this film was just too much\n",
      " Practically nothing was even close to truth, and even for the era, it was seen as silly\n",
      "In defense of ford, it was revealed on an old talk show, that he was operating on the story as told to him by the real Wyatt Earp, who was obviously old, senile, and replayed the scene his own way\n",
      " Earp told the director about the stagecoach, and how it was planned to happen during the stagecoach arrival, so despite what other historians claim, Wyat himself asserts that it was premeditated\n",
      "This movie portrays Earp as an honest man, and also his brothers\n",
      " History doesn't exactly say they were or weren't\n",
      " Most people like to interject a bit of deceit and lawlessness into their characters, but that is nothing new\n",
      " The truth is probably closer to them being the law abiding sorts of GUNFIGHT AT THE OK CORRAL\n",
      " Men who saw it as a career, and believe me, in the old West, you didn't have time to think about too much else\n",
      "Characters that don't exist, characters depicted dying at the corral who really didn't, all make this a weaker film\n",
      " It is further weakened by Mature, who really didn't make a convincing Doc\n",
      " He may be the worst cast choice ever for Doc, but at the same time we must remember that older movies were closer to the era and closer to a feel for the truth\n",
      " After all, ford did get information first hand from Wyatt Earp\n",
      "It is also weakened by the all so predictable events involving the Mexican girl\n",
      " Hollywood was very pro Nazi in those days, and ready to kill off brunette women in very predictable fashion to show their patronage to Hitler idealism\n",
      " This occurs in most movies until the eighties\n",
      " It is no excuse, and does cheapen the art, however\n",
      "The actors who play the Earps do well, and Brennan is always a thrill\n",
      " In fact, Mature may be the only acting downside of this flick\n",
      " Still, it is the weakest of the old OK Corral movies\n",
      "\n",
      "difference: 0.0695\n",
      "difference: 0.16099999999999992\n",
      "difference: 0.9529000000000001\n",
      "Review 10 done.\n",
      "If it weren't for the editing out of curse words and a superimposed blur when one character give another the finger, it would be easy to mistake this low-budget snoozer for a Sci-Fi channel pilot\n",
      " The plot about the government's attempts to destroy a group of telekinetics it originally trained as military weapons ends ambiguously enough with the hero, himself gifted, in pursuit of a telekinetic survivor bent on revenge\n",
      " Alas, the movie is talky, boring, predictable and even devoid of interesting special effects\n",
      " Top-billed Louis Gossett, Jr\n",
      " has a minor role as the evil government bureaucrat who originated the program and now wants to eliminate all traces\n",
      " He walks through the part and it is hard to understand why he bothered\n",
      " Other members of the cast do a decent job with a script that demands little\n",
      "\n",
      "difference: 0.9463\n",
      "difference: 0.5421\n",
      "difference: 0.23450000000000004\n",
      "Review 11 done.\n",
      "I like the good and evil battle\n",
      " I liked Eddie in this movie better than any movie he has ever done\n",
      " He wasn't The smart, cocky, know it all he usually plays\n",
      " He shows heart and a more humble humor\n",
      " The fact that it shows there are stranger things in Heaven and on earth than we can think of gives me hope\n",
      "\n",
      "difference: 0.06730000000000003\n",
      "difference: 0.023599999999999954\n",
      "difference: 0.0686\n",
      "Review 12 done.\n",
      "This crime thriller is sort of like a film noir, though changes the context from post-war to Cold War and has something relatively decent to say about humanity\n",
      " In \"Pickup on South Street\", policemen are good guys, criminals are genuine guys, and the only enemies are \"The Commies\", who are ultimately differentiated from the good-guys in that they are emotionally personable, driven by an actual care for their own worth, as shown in the constant tracked-in close-ups that speckle the movie\n",
      "This movie revolves around characters\n",
      " The personalities in this film are rather unique and detailed: Skip the pick-pocket who is able to stare down any danger, and sometimes while going through their personal possessions; Moe the informer who is just trying to save up for a spectacular funeral, but who manages to capture the hearts and respect of nearly all the other characters (and the audience); Candy, the ill-named innocent girl who only thinks she's doing government work and doesn't fully comprehend the conspiracy she's involved with; and Joey, the ex-boyfriend evil Commie baddie who is trying to hide everything from everybody and, ironically, is the worst person at doing it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Throw in a bunch of very colorful supporting characters (such as the guy with the chopsticks and the policemen) and \"Pickup on South Street\" treats you to a splendor of personalities as they hunt down the mysterious and accidentally stolen microfilm frames\n",
      "--PolarisDiB\n",
      "difference: 0.8823000000000001\n",
      "difference: 0.025800000000000045\n",
      "difference: 0.09720000000000006\n",
      "Review 13 done.\n",
      "I saw the The Bourne Ultimatum last summer with a friend, and, wow! I had already seen the first two films and I liked them, but Ultimatum, I loved\n",
      "Matt Damon plays Jason Bourne, a amnesia suffering CIA agent on the run, trying to discover who he is\n",
      "Like I already said, I loved this movie from start to finish, no plot holes, slow scenes, everything was paced just right and it fit in well with the other films, but in all senses it was much better\n",
      "Best stunts, car chases, actors, and effects I've seen in an action movie all summer, (surprisingly due to Spider-Man 3, Pirates, etc\n",
      ") But I it wasn't just action in this film, Jason doesn't just kill and run\n",
      " He has a soul and the audience feels for him, so drama is included\n",
      " But that doesn't slow it down\n",
      "Of all the \"threequels\" that came out last summer, this was the best\n",
      "\n",
      "difference: 0.07040000000000002\n",
      "difference: 0.045599999999999974\n",
      "difference: 0.00990000000000002\n",
      "Review 14 done.\n",
      "Universal studios\n",
      " The name conjures up so many memories to horror fans of beautiful matte paintings\n",
      "\n",
      "\n",
      "er\n",
      "\n",
      "\n",
      "landscapes, fog-enshrouded countrysides, full moons, howling wolves, taverns and torch wielding mobs\n",
      " Yet, it is quite strange looking back on those films, how little the era which produced the true classics lasted\n",
      " The '30's had it's masterpieces, but those were mostly from the same directors and many of their films are mostly of historical interest today\n",
      " The '40's produced more mindlessly fun matinée films than it did high art, and most fans agree that the classic era of the Gothic horror films ended in 1945 with 'House of Dracula', bumped around with the Abbott & Costello comedies, and was officially dead by the 50's and the coming of the Atomic Age\n",
      "So it's strange that perhaps one of Universal's finest Gothics was made in the '50s! That alone makes the film an oddity, since costume dramas were on their way out EVERYWHERE\n",
      " It may not be comparable to the all time classics like the James Whale films and 'The Wolfman', but damn does it condense all the fun elements of those films into one delicious treat! Get ready for nothing but pure entertainment! Since various social mores had changed, the film also indulges in violence that would not have been allowed in earlier eras; there's no blood, but we see characters shot and stabbed directly, and there's even a burning torch to the face!The film may not really be a horror film, but it sure has the atmosphere, look and feel of one! It begins in the dead of night with howling winds(and wolves) as two men seal the tombs of two apparently dead young lovers, BUT WAIT! One of them is not dead, but he can't move! He's trapped, but talking silently in his own mind, unable to communicate! What is his story? Now THAT is how you start a horror film\n",
      "\n",
      "\n",
      "\n",
      "English businessman Sir Ronald Burton(Richard Greene, giving one of his best performances)sets out under the alias of Richard Beckett to investigate the disappearance of two friends of his who disappeared at the castle of one-eyed Austrian Count Von Bruno(Did Sacha Baron Cohen see this?)\n",
      " It seems many years ago Von Bruno posed as a god to the natives in Africa to steal ivory, he was exposed by Burton's men, and the natives disfigured him\n",
      " Now he is lusting for revenge\n",
      "The film has all the clichés, a Jonathan Harker-style ride to the castle, an inn full of wary villagers, death traps, and a hulking manservant(Lon Chaney Jr\n",
      ")\n",
      " The torture chamber scenes are genuinely suspenseful, including a panel in the floor that activates a dungeon door(which makes for an ingenious getaway scene later on), a hunting expedition involving an imported panther(!!!??)and even an alligator pit! And then there's the Romeo & Juliet-style fake death that leads to the film's prologue\n",
      "\n",
      "\n",
      "\n",
      "The plot is cliché and the world-view is black and white\n",
      " But it's still incredibly entertaining, with director Nathan Juran making every scene pile on the atmosphere and seem fresh and new\n",
      " there are several plot-holes, too\n",
      " Why does Ronald attempt to leave the castle so early? And he never does solve the mystery of what happened to his friends even though it's obvious, but he doesn't know that when he gives up, and he seemed so sure earlier\n",
      " Weird\n",
      "All of the actors are great\n",
      " Despite being a rude upperclassman, Richard Greene makes Burton one of the most likable protagonists I've ever seen in a horror film\n",
      " Rita Corday is excellent as the Count's 'peasant wife' who becomes Burton's love interest, she may have the stereotypical 'falling in love with just a glance' problem of all film heroines of this era, but her portrayal of a long-suffering woman in a loveless marriage to a sadistic monster is very convincing, she also shows a suspicion and wit few film heroines of this era do, even seeing a forced attempt by Burton to quiet her(And to paw at her necklace, although it is simply to analyze it as it has an important clue) as a rape attempt, yet she never seems nasty in her paranoia, the audience sympathizes with her\n",
      " Even though he fails to make Von Bruno seem truly foreign and otherworldly(he is a villain in a horror film after all), Stephen McNally gives a truly chilling performance as the sadistic nobleman\n",
      " When he reveals his burnt eye, and laughs maniacally, it is truly scary\n",
      " Lon Chaney Jr\n",
      " is also great as his mute henchman Gargon\n",
      "But the best performance is undeniably by Boris Karloff as the mysterious physician Dr\n",
      " Meissen\n",
      " He has little screen time at first, but steals every scene he is in\n",
      " It is a performance that is both creepy, sad, mysterious and ultimately, heroic\n",
      " As he leers evilly as he applies leeches to one of Bruno's henchman, spies on our hero and speaks in that famous 'You know I'm up to no good' voice only Karloff could make, you wonder whether if he is a hero or villain, always skulking about\n",
      " And although he does function as the main agent of our hero's escape, he still shows human frailties such as greed and fearfulness for his own life, even cold-bloodedly poisoning a man for Burton's benefit before he even gets Burton to agree\n",
      " His plan to help our hero & heroine escape also puts them directly in the clutches of Von Bruno as he arranges an amazingly sadistic(If unbelievably flawed)death for them that had to have inspired similar scenes in Corman's Poe series\n",
      " He may not have much screen time, but this is easily one of Boris's best performances\n",
      "While the ending may seem abrupt and anti-climatic for some, half the fun is getting there, and there's much excitement to be had\n",
      " Kick back and relive the days when Heroes were heroes and Villains were villains\n",
      " It's no masterpiece, but it's escapist fun at it's best\n",
      "\n",
      "difference: 0.012600000000000056\n",
      "difference: 0.0020999999999999908\n",
      "difference: 0.03990000000000005\n",
      "Review 15 done.\n",
      "I have seen this movie when it was released and no doubt it is heart touching\n",
      " I liked the point of view of a kid who came to know that what she was thinking about her were actually not true\n",
      " It's a shatter to that small kid\n",
      " And her search to find out who she is\n",
      " And before and after she knows about her, the relationship between her and her foster-mom\n",
      " That's a nice view\n",
      " A R Rahman adds his stress by a good re-recording and songs\n",
      " In this movie mani ratnam does not exaggerate or give advices (like in Vuyire) but simply narrates the characters as they are \n",
      " And because of that the film exactly strikes the audience\n",
      " The pool bath scene of chakkarvarthy and nandhitadas did not convey perfectly what it meant for\n",
      " Mani Ratnam has amazingly improved\n",
      "\n",
      "difference: 1.4505\n",
      "difference: 1.299\n",
      "difference: 1.1513\n",
      "Review 16 done.\n",
      "I'm amazed that Memento (which is an excellent flick) is so well-regarded in the Top 250 and this one doesn't even appear!! What the hell is that?? To be honest - when this movie ended my knee-jerk reaction was that this movie is better than Memento\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After the euphoria of the fabulous ending wore off, I concluded that they are equal in their excellence\n",
      " I am just confused about why its not in the Top 50 along with Memento\n",
      " I'm going to venture a guess that (sadly) it's because it's in black and white or because (again sadly)that the characters all have British accents \n",
      "\n",
      "\n",
      "sadly because that is no reason to not appreciate a great movie like this\n",
      "I'm telling you that if you loved Memento, you will love Following as well\n",
      " Brilliant!\n",
      "difference: 0.0021999999999999797\n",
      "difference: 0.04400000000000004\n",
      "difference: 0.06169999999999998\n",
      "Review 17 done.\n",
      "This is the most cliche ridden and worst romantic comedy I have ever seen\n",
      " Every scene is cringe worthy and the two lead actors - Corey and Danny are soo annoying\n",
      " Corey is very dumb and naive and should have never listened to Danny's false promises\n",
      "Neve Campbell and the killer from Urban Legend are the only redeeming qualities in this poor attempt of a film\n",
      " Danny (Dean Paras) looks in his late thirties and the girl he's trying to bed - Corey looks as if she's still in college\n",
      "Here in Australia, this film is called Too Smooth; there is nothing smooth about this film at all\n",
      " 1/10 Avoid\n",
      "difference: 0.46570000000000006\n",
      "difference: 0.0969000000000001\n",
      "difference: 0.007500000000000062\n",
      "Review 18 done.\n",
      "The thing which makes \"Fire\" even more appealing to watch apart from its magical artistry, is its touch of femininism and rebellion\n",
      " To my mind, the very character played by Shabana Azmi is a symbol of the Indian feminine protest against the Indian society\n",
      " The name of the movie and the scene when Radha walks through flames in her kitchen are symbloic of Hindu Mythology's Lord Rama's wife Sita's walking through fire for the proof of her immaculacy, as per the same narrative which appears in the film too\n",
      " The film could be a great inspiration for women, particularly those in the subcontinent, to search for their liberties and to attain control of their lives\n",
      "\n",
      "difference: 0.09830000000000005\n",
      "difference: 0.054300000000000015\n",
      "difference: 0.013299999999999979\n",
      "Review 19 done.\n",
      "The production values for this film make it fall short of Hollywood blockbuster status, and the script makes it fall short of cult status\n",
      " What is left is a tired formulaic attempt at the disaster movie genre that will disappear with the ebb tide\n",
      "A decent cast, are either miss cast, or cannot be bothered\n",
      "The beautiful Joanne Whalley is unable to bring any gravitas to the role of Police Commissioner Nash who wears the most irritating matching waist clincher above her skirt\n",
      "Jessalyn Gilseg plays the heavyweight part of Director of the Thames Barrier with all the conviction of a fairground candy floss\n",
      " Her Canadian nationality and accent were presumably drafted in to appeal to a transatlantic audience\n",
      " It, and she, fails\n",
      "Her initial appearance in a tight fitting pink jogging suit as she arrives at work is risible\n",
      "The part of the \"Siren old git who was right\" is played by Tom Courtenay as though he is acting in his sleep, and the various plot twists that are designed to energise his son, played by Robert Carlyle, struggle to get any response from him\n",
      "Nigel Planer looks determined to commit ritual hari kari for his failings as Met Office Director, or for his acting, or both, and only David Suchet emerges with some credit for his role as Deputy PM\n",
      "There was enough in the story, and the cast and the effects to have produced a decent effort\n",
      " Alas that did not happen\n",
      "\n",
      "difference: 0.6328\n",
      "difference: 0.8461000000000001\n",
      "difference: 1.2324\n",
      "Review 20 done.\n"
     ]
    }
   ],
   "source": [
    "sentence_db = small_db.copy()\n",
    "polarity_s = []\n",
    "\n",
    "reviews = sentence_db['review'].values\n",
    "sentiments = sentence_db['sentiment'].values\n",
    "\n",
    "for i in range(len(reviews)):\n",
    "    base_score = sia.polarity_scores(reviews[i])['compound'] # Get sentiment of original review\n",
    "    sentences = reviews[i].split(\".\") # Separate into sentences\n",
    "\n",
    "    # Get new reviews paraphrased by sentences\n",
    "    res = ['','','']\n",
    "    for s in sentences:\n",
    "        print(s)\n",
    "        para_phrase =paraphrase(s)\n",
    "        for j in range(len(res)):\n",
    "            res[j] += para_phrase[j]\n",
    "\n",
    "    # New review is added to dataframe if score difference is less than 0.2\n",
    "    for rs in res:\n",
    "        para_score = sia.polarity_scores(rs)['compound']\n",
    "        diff = abs(para_score - base_score)\n",
    "        print(f'difference: {diff}')\n",
    "        if diff <= 0.2:\n",
    "            new_row = {'review': rs, 'sentiment': sentiments[i]}\n",
    "            sentence_db = pd.concat([sentence_db, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            polarity_s.append(diff)\n",
    "    print(f'Review {i+1} done.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a2001f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.boxplot(polarity_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dabb4e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    39\n",
       "negative    18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sentence_db))\n",
    "sentence_db[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7451aab",
   "metadata": {},
   "source": [
    "# Whole review wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "94167a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khant\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference: 0.029000000000000026\n",
      "difference: 0.0685\n",
      "difference: 0.06030000000000002\n",
      "difference: 0.039899999999999936\n",
      "Review 1 done.\n",
      "difference: 0.05169999999999997\n",
      "difference: 0.7173\n",
      "difference: 0.28759999999999997\n",
      "difference: 1.1268\n",
      "Review 2 done.\n",
      "difference: 0.014700000000000046\n",
      "difference: 1.3005\n",
      "difference: 0.8806\n",
      "difference: 0.12380000000000002\n",
      "Review 3 done.\n",
      "difference: 0.17049999999999998\n",
      "difference: 0.5858\n",
      "difference: 0.7857999999999999\n",
      "difference: 0.6046\n",
      "Review 4 done.\n",
      "difference: 0.38589999999999997\n",
      "difference: 0.5039\n",
      "difference: 0.4239\n",
      "difference: 0.4541\n",
      "Review 5 done.\n",
      "difference: 0.19310000000000005\n",
      "difference: 0.04049999999999998\n",
      "difference: 0.034700000000000064\n",
      "difference: 0.18369999999999997\n",
      "Review 6 done.\n",
      "difference: 0.3578\n",
      "difference: 0.28259999999999996\n",
      "difference: 0.1078\n",
      "difference: 1.1917\n",
      "Review 7 done.\n",
      "difference: 0.9921\n",
      "difference: 1.0693\n",
      "difference: 1.3739\n",
      "difference: 0.9921\n",
      "Review 8 done.\n",
      "difference: 1.5379999999999998\n",
      "difference: 0.9402999999999999\n",
      "difference: 1.5347\n",
      "difference: 0.4394\n",
      "Review 9 done.\n",
      "difference: 0.2983\n",
      "difference: 1.1628\n",
      "difference: 0.010400000000000076\n",
      "difference: 0.11639999999999995\n",
      "Review 10 done.\n",
      "difference: 0.35230000000000006\n",
      "difference: 1.5057\n",
      "difference: 1.2438\n",
      "difference: 0.1481\n",
      "Review 11 done.\n",
      "difference: 0.18030000000000002\n",
      "difference: 0.13650000000000007\n",
      "difference: 0.13650000000000007\n",
      "difference: 0.08099999999999996\n",
      "Review 12 done.\n",
      "difference: 0.442\n",
      "difference: 0.08079999999999998\n",
      "difference: 0.6476999999999999\n",
      "difference: 0.738\n",
      "Review 13 done.\n",
      "difference: 0.9171\n",
      "difference: 0.42510000000000003\n",
      "difference: 1.0486\n",
      "difference: 1.3286\n",
      "Review 14 done.\n",
      "difference: 1.2457\n",
      "difference: 1.1865999999999999\n",
      "difference: 0.13479999999999992\n",
      "difference: 0.4576\n",
      "Review 15 done.\n",
      "difference: 0.28059999999999996\n",
      "difference: 0.395\n",
      "difference: 0.05930000000000002\n",
      "difference: 0.2901\n",
      "Review 16 done.\n",
      "difference: 0.8840000000000001\n",
      "difference: 0.7637\n",
      "difference: 1.3032\n",
      "difference: 0.0414000000000001\n",
      "Review 17 done.\n",
      "difference: 1.3018\n",
      "difference: 0.3642\n",
      "difference: 0.3494\n",
      "difference: 1.2330999999999999\n",
      "Review 18 done.\n",
      "difference: 0.33220000000000005\n",
      "difference: 0.5493\n",
      "difference: 1.6804999999999999\n",
      "difference: 1.8686\n",
      "Review 19 done.\n",
      "difference: 0.12279999999999991\n",
      "difference: 0.12279999999999991\n",
      "difference: 0.18489999999999995\n",
      "difference: 0.015199999999999991\n",
      "Review 20 done.\n",
      "difference: 1.1421999999999999\n",
      "difference: 1.3942\n",
      "difference: 0.9157\n",
      "difference: 1.6301\n",
      "Review 21 done.\n",
      "difference: 0.0784999999999999\n",
      "difference: 0.08310000000000006\n",
      "difference: 0.7584000000000001\n",
      "difference: 1.0123\n",
      "Review 22 done.\n",
      "difference: 0.315\n",
      "difference: 0.7904\n",
      "difference: 0.0121\n",
      "difference: 0.35\n",
      "Review 23 done.\n",
      "difference: 0.2056\n",
      "difference: 0.9962\n",
      "difference: 0.2056\n",
      "difference: 0.0524\n",
      "Review 24 done.\n",
      "difference: 0.2988\n",
      "difference: 0.03390000000000004\n",
      "difference: 0.15850000000000003\n",
      "difference: 0.29500000000000004\n",
      "Review 25 done.\n",
      "difference: 0.32110000000000005\n",
      "difference: 1.2494\n",
      "difference: 0.31790000000000007\n",
      "difference: 1.2369\n",
      "Review 26 done.\n",
      "difference: 1.8018\n",
      "difference: 1.4686\n",
      "difference: 1.3565\n",
      "difference: 0.1431\n",
      "Review 27 done.\n",
      "difference: 0.5625\n",
      "difference: 0.538\n",
      "difference: 0.5784\n",
      "difference: 0.6234\n",
      "Review 28 done.\n",
      "difference: 0.8801\n",
      "difference: 0.3094\n",
      "difference: 0.9691\n",
      "difference: 0.08079999999999998\n",
      "Review 29 done.\n",
      "difference: 1.613\n",
      "difference: 0.4071\n",
      "difference: 1.2951\n",
      "difference: 0.4994\n",
      "Review 30 done.\n",
      "difference: 0.4252\n",
      "difference: 0.09699999999999998\n",
      "difference: 0.34850000000000003\n",
      "difference: 0.013000000000000012\n",
      "Review 31 done.\n",
      "difference: 1.3155999999999999\n",
      "difference: 0.8464\n",
      "difference: 1.4621\n",
      "difference: 0.7437\n",
      "Review 32 done.\n",
      "difference: 0.4478000000000001\n",
      "difference: 0.06000000000000005\n",
      "difference: 0.39560000000000006\n",
      "difference: 0.05249999999999999\n",
      "Review 33 done.\n",
      "difference: 0.15369999999999995\n",
      "difference: 0.347\n",
      "difference: 0.055299999999999905\n",
      "difference: 0.12619999999999998\n",
      "Review 34 done.\n",
      "difference: 0.5495\n",
      "difference: 0.1946\n",
      "difference: 1.0209000000000001\n",
      "difference: 1.1794\n",
      "Review 35 done.\n",
      "difference: 0.9543\n",
      "difference: 0.7444000000000001\n",
      "difference: 0.03310000000000002\n",
      "difference: 1.2666\n",
      "Review 36 done.\n",
      "difference: 0.44530000000000003\n",
      "difference: 0.7376\n",
      "difference: 0.4937\n",
      "difference: 1.6245\n",
      "Review 37 done.\n",
      "difference: 0.5337\n",
      "difference: 0.7186\n",
      "difference: 0.5071\n",
      "difference: 0.3238\n",
      "Review 38 done.\n",
      "difference: 1.3877\n",
      "difference: 0.16159999999999997\n",
      "difference: 1.5165000000000002\n",
      "difference: 0.015700000000000047\n",
      "Review 39 done.\n",
      "difference: 1.0608\n",
      "difference: 0.9003\n",
      "difference: 1.6653\n",
      "difference: 0.7472\n",
      "Review 40 done.\n",
      "difference: 1.3135\n",
      "difference: 1.6135\n",
      "difference: 1.6909\n",
      "difference: 0.6734\n",
      "Review 41 done.\n",
      "difference: 0.8488\n",
      "difference: 0.5442\n",
      "difference: 0.9657\n",
      "difference: 1.7166000000000001\n",
      "Review 42 done.\n",
      "difference: 1.4148\n",
      "difference: 1.3959000000000001\n",
      "difference: 1.6840000000000002\n",
      "difference: 1.6552\n",
      "Review 43 done.\n",
      "difference: 0.15539999999999998\n",
      "difference: 0.31389999999999996\n",
      "difference: 0.261\n",
      "difference: 1.7534999999999998\n",
      "Review 44 done.\n",
      "difference: 0.004699999999999982\n",
      "difference: 0.8168\n",
      "difference: 0.34559999999999996\n",
      "difference: 0.3703\n",
      "Review 45 done.\n",
      "difference: 0.10319999999999996\n",
      "difference: 0.2599\n",
      "difference: 1.2548\n",
      "difference: 0.23209999999999997\n",
      "Review 46 done.\n",
      "difference: 0.07479999999999998\n",
      "difference: 0.020799999999999985\n",
      "difference: 0.4357\n",
      "difference: 0.7458\n",
      "Review 47 done.\n",
      "difference: 0.09499999999999997\n",
      "difference: 0.06829999999999992\n",
      "difference: 0.15359999999999996\n",
      "difference: 0.24539999999999995\n",
      "Review 48 done.\n",
      "difference: 0.25039999999999996\n",
      "difference: 0.02300000000000002\n",
      "difference: 0.25039999999999996\n",
      "difference: 0.15000000000000002\n",
      "Review 49 done.\n",
      "difference: 0.4869\n",
      "difference: 0.6626000000000001\n",
      "difference: 1.6942\n",
      "difference: 1.1088\n",
      "Review 50 done.\n",
      "difference: 0.052000000000000046\n",
      "difference: 0.31720000000000004\n",
      "difference: 0.1614\n",
      "difference: 0.1411\n",
      "Review 51 done.\n",
      "difference: 0.1926\n",
      "difference: 0.4779\n",
      "difference: 0.1926\n",
      "difference: 0.5642\n",
      "Review 52 done.\n",
      "difference: 0.12569999999999992\n",
      "difference: 0.8337\n",
      "difference: 1.0211\n",
      "difference: 1.3086\n",
      "Review 53 done.\n",
      "difference: 0.5089999999999999\n",
      "difference: 0.39879999999999993\n",
      "difference: 0.20119999999999993\n",
      "difference: 1.1377\n",
      "Review 54 done.\n",
      "difference: 0.9076\n",
      "difference: 0.23939999999999995\n",
      "difference: 0.18209999999999993\n",
      "difference: 0.07050000000000001\n",
      "Review 55 done.\n",
      "difference: 0.6392\n",
      "difference: 0.12729999999999997\n",
      "difference: 0.12239999999999995\n",
      "difference: 0.19469999999999998\n",
      "Review 56 done.\n",
      "difference: 0.05510000000000004\n",
      "difference: 0.05510000000000004\n",
      "difference: 0.3863\n",
      "difference: 0.7359\n",
      "Review 57 done.\n",
      "difference: 0.07930000000000004\n",
      "difference: 0.04159999999999997\n",
      "difference: 1.0511\n",
      "difference: 1.2599\n",
      "Review 58 done.\n",
      "difference: 0.07369999999999999\n",
      "difference: 1.1421\n",
      "difference: 1.5154\n",
      "difference: 1.4042\n",
      "Review 59 done.\n",
      "difference: 0.14590000000000003\n",
      "difference: 0.13\n",
      "difference: 0.14590000000000003\n",
      "difference: 0.11560000000000004\n",
      "Review 60 done.\n",
      "difference: 0.0484\n",
      "difference: 0.1875\n",
      "difference: 0.29009999999999997\n",
      "difference: 0.4221\n",
      "Review 61 done.\n",
      "difference: 0.019600000000000062\n",
      "difference: 0.026700000000000057\n",
      "difference: 0.9399000000000001\n",
      "difference: 0.055499999999999994\n",
      "Review 62 done.\n",
      "difference: 0.5098\n",
      "difference: 0.1069\n",
      "difference: 0.6347\n",
      "difference: 0.07730000000000004\n",
      "Review 63 done.\n",
      "difference: 1.254\n",
      "difference: 0.03969999999999996\n",
      "difference: 0.01529999999999998\n",
      "difference: 0.09640000000000004\n",
      "Review 64 done.\n",
      "difference: 0.8721\n",
      "difference: 1.2635999999999998\n",
      "difference: 1.4541\n",
      "difference: 1.0394999999999999\n",
      "Review 65 done.\n",
      "difference: 0.9663\n",
      "difference: 0.5702\n",
      "difference: 0.6029\n",
      "difference: 0.22150000000000003\n",
      "Review 66 done.\n",
      "difference: 0.4685\n",
      "difference: 1.7554\n",
      "difference: 0.9717\n",
      "difference: 1.7730000000000001\n",
      "Review 67 done.\n",
      "difference: 0.3148000000000001\n",
      "difference: 0.07550000000000001\n",
      "difference: 0.7137\n",
      "difference: 0.43820000000000003\n",
      "Review 68 done.\n",
      "difference: 0.129\n",
      "difference: 0.027000000000000024\n",
      "difference: 0.1261\n",
      "difference: 0.09299999999999997\n",
      "Review 69 done.\n",
      "difference: 0.17510000000000003\n",
      "difference: 0.2633000000000001\n",
      "difference: 0.5561\n",
      "difference: 0.43720000000000003\n",
      "Review 70 done.\n",
      "difference: 0.9346\n",
      "difference: 1.012\n",
      "difference: 0.4337000000000001\n",
      "difference: 0.8379000000000001\n",
      "Review 71 done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference: 1.3045\n",
      "difference: 0.20440000000000003\n",
      "difference: 0.2922\n",
      "difference: 0.10619999999999996\n",
      "Review 72 done.\n",
      "difference: 0.07520000000000004\n",
      "difference: 0.03499999999999992\n",
      "difference: 0.3162\n",
      "difference: 0.6344\n",
      "Review 73 done.\n",
      "difference: 1.0498\n",
      "difference: 0.32130000000000003\n",
      "difference: 1.8193000000000001\n",
      "difference: 0.9495\n",
      "Review 74 done.\n",
      "difference: 0.10170000000000001\n",
      "difference: 0.7559\n",
      "difference: 0.12220000000000009\n",
      "difference: 0.01860000000000006\n",
      "Review 75 done.\n",
      "difference: 1.1805999999999999\n",
      "difference: 0.476\n",
      "difference: 0.3596999999999999\n",
      "difference: 1.3539999999999999\n",
      "Review 76 done.\n",
      "difference: 0.024800000000000044\n",
      "difference: 0.7882\n",
      "difference: 0.10340000000000005\n",
      "difference: 0.19669999999999999\n",
      "Review 77 done.\n",
      "difference: 0.1612\n",
      "difference: 0.34650000000000003\n",
      "difference: 0.17510000000000003\n",
      "difference: 0.19289999999999996\n",
      "Review 78 done.\n",
      "difference: 0.15249999999999997\n",
      "difference: 0.47859999999999997\n",
      "difference: 0.09060000000000001\n",
      "difference: 0.29259999999999997\n",
      "Review 79 done.\n",
      "difference: 1.1103\n",
      "difference: 0.6647\n",
      "difference: 1.2604\n",
      "difference: 0.8873\n",
      "Review 80 done.\n",
      "difference: 1.5469\n",
      "difference: 1.5756999999999999\n",
      "difference: 1.297\n",
      "difference: 0.21829999999999994\n",
      "Review 81 done.\n",
      "difference: 0.6914\n",
      "difference: 0.017199999999999993\n",
      "difference: 0.5545\n",
      "difference: 0.01969999999999994\n",
      "Review 82 done.\n",
      "difference: 1.127\n",
      "difference: 0.44910000000000005\n",
      "difference: 0.10310000000000002\n",
      "difference: 0.668\n",
      "Review 83 done.\n",
      "difference: 1.2933\n",
      "difference: 1.2307\n",
      "difference: 1.2859\n",
      "difference: 1.0559\n",
      "Review 84 done.\n",
      "difference: 0.5006\n",
      "difference: 0.21620000000000006\n",
      "difference: 0.6763000000000001\n",
      "difference: 0.7213\n",
      "Review 85 done.\n",
      "difference: 0.35309999999999997\n",
      "difference: 0.5204\n",
      "difference: 0.3294\n",
      "difference: 0.29059999999999997\n",
      "Review 86 done.\n",
      "difference: 0.032200000000000006\n",
      "difference: 0.11880000000000002\n",
      "difference: 0.05879999999999996\n",
      "difference: 0.29180000000000006\n",
      "Review 87 done.\n",
      "difference: 1.0362\n",
      "difference: 0.3296\n",
      "difference: 1.0905\n",
      "difference: 0.7958000000000001\n",
      "Review 88 done.\n",
      "difference: 0.6788000000000001\n",
      "difference: 0.39\n",
      "difference: 0.37160000000000004\n",
      "difference: 0.3548\n",
      "Review 89 done.\n",
      "difference: 0.5466\n",
      "difference: 1.1101\n",
      "difference: 0.7489\n",
      "difference: 1.3604\n",
      "Review 90 done.\n",
      "difference: 1.1831\n",
      "difference: 1.2348\n",
      "difference: 0.6775\n",
      "difference: 1.348\n",
      "Review 91 done.\n",
      "difference: 1.6016\n",
      "difference: 0.04159999999999997\n",
      "difference: 0.09400000000000008\n",
      "difference: 0.9629\n",
      "Review 92 done.\n",
      "difference: 0.28170000000000006\n",
      "difference: 0.8282\n",
      "difference: 0.1503000000000001\n",
      "difference: 1.2022\n",
      "Review 93 done.\n",
      "difference: 0.0938\n",
      "difference: 0.05349999999999999\n",
      "difference: 0.032400000000000095\n",
      "difference: 0.2389\n",
      "Review 94 done.\n",
      "difference: 0.923\n",
      "difference: 0.1191000000000001\n",
      "difference: 0.20690000000000008\n",
      "difference: 1.5951\n",
      "Review 95 done.\n",
      "difference: 0.338\n",
      "difference: 0.5334000000000001\n",
      "difference: 0.6054999999999999\n",
      "difference: 0.5156000000000001\n",
      "Review 96 done.\n",
      "difference: 0.08340000000000003\n",
      "difference: 0.08289999999999997\n",
      "difference: 0.06879999999999997\n",
      "difference: 0.3244\n",
      "Review 97 done.\n",
      "difference: 0.08779999999999999\n",
      "difference: 0.08779999999999999\n",
      "difference: 0.318\n",
      "difference: 0.27740000000000004\n",
      "Review 98 done.\n",
      "difference: 0.4639\n",
      "difference: 0.1663\n",
      "difference: 9.999999999998899e-05\n",
      "difference: 0.0837\n",
      "Review 99 done.\n",
      "difference: 0.035700000000000065\n",
      "difference: 0.5796\n",
      "difference: 0.21010000000000006\n",
      "difference: 0.19420000000000004\n",
      "Review 100 done.\n",
      "difference: 0.07989999999999997\n",
      "difference: 0.14390000000000003\n",
      "difference: 0.6440999999999999\n",
      "difference: 0.1935\n",
      "Review 101 done.\n",
      "difference: 0.3716999999999999\n",
      "difference: 1.603\n",
      "difference: 0.12459999999999993\n",
      "difference: 0.7172\n",
      "Review 102 done.\n",
      "difference: 0.07610000000000006\n",
      "difference: 0.431\n",
      "difference: 0.07069999999999999\n",
      "difference: 0.0474\n",
      "Review 103 done.\n",
      "difference: 1.0417\n",
      "difference: 0.6223000000000001\n",
      "difference: 0.3146\n",
      "difference: 0.4434\n",
      "Review 104 done.\n",
      "difference: 0.04219999999999996\n",
      "difference: 0.07229999999999998\n",
      "difference: 0.2424\n",
      "difference: 0.9366\n",
      "Review 105 done.\n",
      "difference: 0.3578\n",
      "difference: 0.42980000000000007\n",
      "difference: 0.1451\n",
      "difference: 0.42980000000000007\n",
      "Review 106 done.\n",
      "difference: 0.06269999999999998\n",
      "difference: 0.2407999999999999\n",
      "difference: 0.04459999999999997\n",
      "difference: 0.0343\n",
      "Review 107 done.\n",
      "difference: 0.06159999999999999\n",
      "difference: 0.13129999999999997\n",
      "difference: 0.000500000000000056\n",
      "difference: 1.4287999999999998\n",
      "Review 108 done.\n",
      "difference: 0.7898999999999999\n",
      "difference: 0.6521999999999999\n",
      "difference: 0.9922\n",
      "difference: 0.754\n",
      "Review 109 done.\n",
      "difference: 0.9351\n",
      "difference: 0.9351\n",
      "difference: 0.4245\n",
      "difference: 0.5136000000000001\n",
      "Review 110 done.\n",
      "difference: 0.9989\n",
      "difference: 1.5255999999999998\n",
      "difference: 0.17180000000000006\n",
      "difference: 0.39949999999999997\n",
      "Review 111 done.\n",
      "difference: 0.4709\n",
      "difference: 0.2348\n",
      "difference: 0.41059999999999997\n",
      "difference: 1.1207\n",
      "Review 112 done.\n",
      "difference: 0.0522999999999999\n",
      "difference: 1.4419\n",
      "difference: 0.5075999999999999\n",
      "difference: 0.20499999999999996\n",
      "Review 113 done.\n",
      "difference: 0.26160000000000005\n",
      "difference: 0.30690000000000006\n",
      "difference: 0.9927\n",
      "difference: 0.9927\n",
      "Review 114 done.\n",
      "difference: 1.4874\n",
      "difference: 0.04919999999999991\n",
      "difference: 0.24669999999999997\n",
      "difference: 0.45499999999999996\n",
      "Review 115 done.\n",
      "difference: 0.40059999999999996\n",
      "difference: 0.19490000000000002\n",
      "difference: 0.0373\n",
      "difference: 0.26749999999999996\n",
      "Review 116 done.\n",
      "difference: 0.0383\n",
      "difference: 0.07750000000000001\n",
      "difference: 0.47419999999999995\n",
      "difference: 0.021500000000000075\n",
      "Review 117 done.\n",
      "difference: 0.06499999999999995\n",
      "difference: 0.17400000000000004\n",
      "difference: 0.5809\n",
      "difference: 0.1502\n",
      "Review 118 done.\n",
      "difference: 1.0307\n",
      "difference: 0.28090000000000004\n",
      "difference: 0.5713\n",
      "difference: 1.2722\n",
      "Review 119 done.\n",
      "difference: 0.8785999999999999\n",
      "difference: 0.9297\n",
      "difference: 0.07469999999999999\n",
      "difference: 1.2697\n",
      "Review 120 done.\n",
      "difference: 0.12629999999999997\n",
      "difference: 0.16049999999999998\n",
      "difference: 0.04139999999999999\n",
      "difference: 0.05840000000000001\n",
      "Review 121 done.\n",
      "difference: 0.2944\n",
      "difference: 0.23229999999999995\n",
      "difference: 0.32919999999999994\n",
      "difference: 0.3485999999999999\n",
      "Review 122 done.\n",
      "difference: 0.14670000000000005\n",
      "difference: 0.48029999999999995\n",
      "difference: 0.27249999999999996\n",
      "difference: 1.3521\n",
      "Review 123 done.\n",
      "difference: 0.08510000000000006\n",
      "difference: 1.1686\n",
      "difference: 0.01539999999999997\n",
      "difference: 0.17059999999999997\n",
      "Review 124 done.\n",
      "difference: 0.398\n",
      "difference: 0.9894000000000001\n",
      "difference: 0.6196\n",
      "difference: 0.6712\n",
      "Review 125 done.\n",
      "difference: 0.03810000000000002\n",
      "difference: 0.1070000000000001\n",
      "difference: 0.07990000000000008\n",
      "difference: 0.057499999999999996\n",
      "Review 126 done.\n",
      "difference: 0.1402\n",
      "difference: 0.09809999999999997\n",
      "difference: 0.04689999999999994\n",
      "difference: 0.4311999999999999\n",
      "Review 127 done.\n",
      "difference: 0.5127999999999999\n",
      "difference: 0.9883\n",
      "difference: 0.1884\n",
      "difference: 0.8178\n",
      "Review 128 done.\n",
      "difference: 0.2007\n",
      "difference: 0.38789999999999997\n",
      "difference: 0.29159999999999997\n",
      "difference: 0.09260000000000002\n",
      "Review 129 done.\n",
      "difference: 1.7106\n",
      "difference: 1.5464\n",
      "difference: 0.8693000000000001\n",
      "difference: 1.6596000000000002\n",
      "Review 130 done.\n",
      "difference: 1.1025\n",
      "difference: 1.4082\n",
      "difference: 1.5646\n",
      "difference: 0.17110000000000003\n",
      "Review 131 done.\n",
      "difference: 0.09620000000000006\n",
      "difference: 0.09920000000000007\n",
      "difference: 0.06769999999999998\n",
      "difference: 0.0796\n",
      "Review 132 done.\n",
      "difference: 0.2701\n",
      "difference: 0.07550000000000001\n",
      "difference: 0.2036\n",
      "difference: 0.025499999999999967\n",
      "Review 133 done.\n",
      "difference: 0.03749999999999998\n",
      "difference: 0.7869\n",
      "difference: 0.05889999999999995\n",
      "difference: 1.7375\n",
      "Review 134 done.\n",
      "difference: 1.1619\n",
      "difference: 0.37829999999999997\n",
      "difference: 0.39239999999999997\n",
      "difference: 0.03079999999999994\n",
      "Review 135 done.\n",
      "difference: 0.2731\n",
      "difference: 0.15880000000000005\n",
      "difference: 0.0361999999999999\n",
      "difference: 0.595\n",
      "Review 136 done.\n",
      "difference: 1.5647\n",
      "difference: 0.9148999999999999\n",
      "difference: 0.04359999999999997\n",
      "difference: 0.46989999999999993\n",
      "Review 137 done.\n",
      "difference: 0.479\n",
      "difference: 0.25770000000000004\n",
      "difference: 0.6765\n",
      "difference: 1.2511\n",
      "Review 138 done.\n",
      "difference: 0.9618\n",
      "difference: 1.5987\n",
      "difference: 1.8368\n",
      "difference: 0.5214\n",
      "Review 139 done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference: 0.14800000000000002\n",
      "difference: 0.1452\n",
      "difference: 0.04749999999999999\n",
      "difference: 0.13280000000000003\n",
      "Review 140 done.\n",
      "difference: 0.5983\n",
      "difference: 0.12160000000000004\n",
      "difference: 0.7137\n",
      "difference: 1.3413\n",
      "Review 141 done.\n",
      "difference: 0.7388\n",
      "difference: 0.8919\n",
      "difference: 0.02939999999999998\n",
      "difference: 0.06480000000000008\n",
      "Review 142 done.\n",
      "difference: 0.6535\n",
      "difference: 0.0816\n",
      "difference: 0.2717\n",
      "difference: 0.6535\n",
      "Review 143 done.\n",
      "difference: 0.36019999999999996\n",
      "difference: 1.0997999999999999\n",
      "difference: 1.1251\n",
      "difference: 0.39769999999999994\n",
      "Review 144 done.\n",
      "difference: 1.5925\n",
      "difference: 1.6966\n",
      "difference: 1.283\n",
      "difference: 0.46319999999999995\n",
      "Review 145 done.\n",
      "difference: 0.46690000000000004\n",
      "difference: 0.37439999999999996\n",
      "difference: 0.4462\n",
      "difference: 1.0598\n",
      "Review 146 done.\n",
      "difference: 0.38670000000000004\n",
      "difference: 1.3198\n",
      "difference: 0.5960000000000001\n",
      "difference: 1.9168\n",
      "Review 147 done.\n",
      "difference: 0.1053\n",
      "difference: 1.1221\n",
      "difference: 0.44260000000000005\n",
      "difference: 0.5166\n",
      "Review 148 done.\n",
      "difference: 0.4254\n",
      "difference: 0.2861\n",
      "difference: 0.3418\n",
      "difference: 0.35429999999999995\n",
      "Review 149 done.\n",
      "difference: 0.5103\n",
      "difference: 0.9346000000000001\n",
      "difference: 0.08430000000000004\n",
      "difference: 0.36240000000000006\n",
      "Review 150 done.\n",
      "difference: 0.9339\n",
      "difference: 0.6156999999999999\n",
      "difference: 0.9339\n",
      "difference: 0.20199999999999996\n",
      "Review 151 done.\n",
      "difference: 0.5605\n",
      "difference: 0.5605\n",
      "difference: 0.38259999999999994\n",
      "difference: 0.2551\n",
      "Review 152 done.\n",
      "difference: 0.3226\n",
      "difference: 1.3413\n",
      "difference: 0.21650000000000003\n",
      "difference: 0.9853000000000001\n",
      "Review 153 done.\n",
      "difference: 0.6938\n",
      "difference: 0.34529999999999994\n",
      "difference: 0.022699999999999942\n",
      "difference: 0.13079999999999992\n",
      "Review 154 done.\n",
      "difference: 0.5182\n",
      "difference: 0.2728999999999999\n",
      "difference: 1.428\n",
      "difference: 1.428\n",
      "Review 155 done.\n",
      "difference: 0.018299999999999983\n",
      "difference: 0.0726\n",
      "difference: 0.029200000000000004\n",
      "difference: 0.08460000000000001\n",
      "Review 156 done.\n",
      "difference: 0.11730000000000007\n",
      "difference: 0.15290000000000004\n",
      "difference: 0.032200000000000006\n",
      "difference: 0.03490000000000004\n",
      "Review 157 done.\n",
      "difference: 1.3947\n",
      "difference: 0.13890000000000002\n",
      "difference: 1.0062\n",
      "difference: 1.3111000000000002\n",
      "Review 158 done.\n",
      "difference: 1.5759\n",
      "difference: 0.8763000000000001\n",
      "difference: 1.5759\n",
      "difference: 0.12460000000000004\n",
      "Review 159 done.\n",
      "difference: 1.2635\n",
      "difference: 0.07219999999999993\n",
      "difference: 0.47769999999999996\n",
      "difference: 0.11880000000000002\n",
      "Review 160 done.\n",
      "difference: 0.6792\n",
      "difference: 0.7486999999999999\n",
      "difference: 0.08519999999999994\n",
      "difference: 1.3147\n",
      "Review 161 done.\n",
      "difference: 0.0233\n",
      "difference: 0.025300000000000003\n",
      "difference: 0.7684\n",
      "difference: 0.2054\n",
      "Review 162 done.\n",
      "difference: 0.12390000000000001\n",
      "difference: 1.6248\n",
      "difference: 1.5406\n",
      "difference: 1.1458\n",
      "Review 163 done.\n",
      "difference: 0.13359999999999994\n",
      "difference: 0.17369999999999997\n",
      "difference: 0.22929999999999995\n",
      "difference: 0.3369\n",
      "Review 164 done.\n",
      "difference: 0.16900000000000004\n",
      "difference: 0.23160000000000003\n",
      "difference: 0.07530000000000003\n",
      "difference: 0.11749999999999994\n",
      "Review 165 done.\n",
      "difference: 1.327\n",
      "difference: 1.8275\n",
      "difference: 1.7238\n",
      "difference: 0.8904\n",
      "Review 166 done.\n",
      "difference: 0.23719999999999997\n",
      "difference: 1.1082\n",
      "difference: 1.2762\n",
      "difference: 0.11129999999999995\n",
      "Review 167 done.\n",
      "difference: 0.11919999999999997\n",
      "difference: 0.32999999999999996\n",
      "difference: 0.2066\n",
      "difference: 0.13679999999999992\n",
      "Review 168 done.\n",
      "difference: 0.009000000000000008\n",
      "difference: 0.04090000000000005\n",
      "difference: 0.006400000000000072\n",
      "difference: 0.16100000000000003\n",
      "Review 169 done.\n",
      "difference: 0.005800000000000027\n",
      "difference: 0.06369999999999998\n",
      "difference: 0.31389999999999996\n",
      "difference: 0.01640000000000008\n",
      "Review 170 done.\n",
      "difference: 0.8356\n",
      "difference: 0.41479999999999995\n",
      "difference: 1.1747\n",
      "difference: 1.5630000000000002\n",
      "Review 171 done.\n",
      "difference: 0.10519999999999996\n",
      "difference: 0.2799999999999999\n",
      "difference: 0.050799999999999956\n",
      "difference: 0.0532999999999999\n",
      "Review 172 done.\n",
      "difference: 0.5349\n",
      "difference: 0.19179999999999997\n",
      "difference: 0.05600000000000005\n",
      "difference: 0.024800000000000044\n",
      "Review 173 done.\n",
      "difference: 0.9611000000000001\n",
      "difference: 0.9248000000000001\n",
      "difference: 1.0267\n",
      "difference: 0.9611000000000001\n",
      "Review 174 done.\n",
      "difference: 0.39270000000000005\n",
      "difference: 0.23239999999999997\n",
      "difference: 1.0278\n",
      "difference: 0.7794\n",
      "Review 175 done.\n",
      "difference: 0.7267\n",
      "difference: 0.18630000000000002\n",
      "difference: 0.2671\n",
      "difference: 0.18610000000000004\n",
      "Review 176 done.\n",
      "difference: 1.7641\n",
      "difference: 1.7066\n",
      "difference: 1.5894\n",
      "difference: 1.7431999999999999\n",
      "Review 177 done.\n",
      "difference: 0.1018\n",
      "difference: 0.663\n",
      "difference: 0.22660000000000002\n",
      "difference: 0.14560000000000006\n",
      "Review 178 done.\n",
      "difference: 0.5771999999999999\n",
      "difference: 1.5884\n",
      "difference: 1.0917\n",
      "difference: 1.117\n",
      "Review 179 done.\n",
      "difference: 1.4794\n",
      "difference: 1.6174\n",
      "difference: 1.6174\n",
      "difference: 1.8004\n",
      "Review 180 done.\n",
      "difference: 1.3417\n",
      "difference: 1.6631\n",
      "difference: 0.8053999999999999\n",
      "difference: 1.1000999999999999\n",
      "Review 181 done.\n",
      "difference: 0.1935\n",
      "difference: 0.11360000000000003\n",
      "difference: 0.12209999999999999\n",
      "difference: 0.0826\n",
      "Review 182 done.\n",
      "difference: 1.309\n",
      "difference: 0.6295999999999999\n",
      "difference: 1.4586000000000001\n",
      "difference: 0.32030000000000003\n",
      "Review 183 done.\n",
      "difference: 0.17320000000000002\n",
      "difference: 0.012499999999999956\n",
      "difference: 0.03520000000000001\n",
      "difference: 0.060799999999999965\n",
      "Review 184 done.\n",
      "difference: 0.05159999999999998\n",
      "difference: 1.2829000000000002\n",
      "difference: 0.7222000000000001\n",
      "difference: 0.46370000000000006\n",
      "Review 185 done.\n",
      "difference: 0.15750000000000008\n",
      "difference: 0.06380000000000008\n",
      "difference: 0.056700000000000084\n",
      "difference: 0.1170000000000001\n",
      "Review 186 done.\n",
      "difference: 0.16820000000000002\n",
      "difference: 0.07950000000000002\n",
      "difference: 0.0746\n",
      "difference: 0.007600000000000051\n",
      "Review 187 done.\n",
      "difference: 0.2277\n",
      "difference: 0.5805\n",
      "difference: 1.1029\n",
      "difference: 0.5952000000000001\n",
      "Review 188 done.\n",
      "difference: 0.11709999999999998\n",
      "difference: 0.12770000000000004\n",
      "difference: 0.20789999999999997\n",
      "difference: 0.6906000000000001\n",
      "Review 189 done.\n",
      "difference: 0.26449999999999996\n",
      "difference: 0.06419999999999992\n",
      "difference: 0.028100000000000014\n",
      "difference: 0.15889999999999993\n",
      "Review 190 done.\n",
      "difference: 1.7409\n",
      "difference: 1.3549\n",
      "difference: 0.15839999999999999\n",
      "difference: 0.5960000000000001\n",
      "Review 191 done.\n",
      "difference: 0.0655\n",
      "difference: 0.13219999999999998\n",
      "difference: 0.0242\n",
      "difference: 0.014399999999999968\n",
      "Review 192 done.\n",
      "difference: 0.2593000000000001\n",
      "difference: 0.9724\n",
      "difference: 0.38670000000000004\n",
      "difference: 1.4626000000000001\n",
      "Review 193 done.\n",
      "difference: 0.41990000000000005\n",
      "difference: 0.15760000000000007\n",
      "difference: 0.1411\n",
      "difference: 0.28400000000000003\n",
      "Review 194 done.\n",
      "difference: 1.4245999999999999\n",
      "difference: 1.7106\n",
      "difference: 0.5389\n",
      "difference: 0.8113\n",
      "Review 195 done.\n",
      "difference: 0.32799999999999996\n",
      "difference: 0.5242\n",
      "difference: 0.3185\n",
      "difference: 0.2783\n",
      "Review 196 done.\n",
      "difference: 0.6675\n",
      "difference: 0.5222\n",
      "difference: 0.1473\n",
      "difference: 0.3304\n",
      "Review 197 done.\n",
      "difference: 1.7127\n",
      "difference: 0.6741999999999999\n",
      "difference: 1.1611\n",
      "difference: 0.6674\n",
      "Review 198 done.\n",
      "difference: 0.42390000000000005\n",
      "difference: 0.005299999999999971\n",
      "difference: 0.2098000000000001\n",
      "difference: 1.2555\n",
      "Review 199 done.\n",
      "difference: 0.20010000000000006\n",
      "difference: 1.8639000000000001\n",
      "difference: 0.3057000000000001\n",
      "difference: 1.8065000000000002\n",
      "Review 200 done.\n"
     ]
    }
   ],
   "source": [
    "para_db = small_db.copy()\n",
    "polarity_p = []\n",
    "\n",
    "reviews = para_db['review'].values\n",
    "sentiments = para_db['sentiment'].values\n",
    "\n",
    "for i in range(len(reviews)):\n",
    "    base_score = sia.polarity_scores(reviews[i])['compound'] # Get sentiment of original review\n",
    "\n",
    "    # New review is added to dataframe if score difference is less than 0.2\n",
    "    new_reviews = paraphrase(reviews[i])\n",
    "    for new_r in new_reviews:\n",
    "        para_score = sia.polarity_scores(new_r)['compound']\n",
    "        diff = abs(para_score - base_score)\n",
    "        print(f'difference: {diff}')\n",
    "        if diff <= 0.2:\n",
    "            new_row = {'review': new_r, 'sentiment': sentiments[i]}\n",
    "            para_db = pd.concat([para_db, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            polarity_p.append(diff)\n",
    "    print(f'Review {i+1} done.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3768b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    285\n",
       "negative    187\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(para_db))\n",
    "para_db[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd79d2",
   "metadata": {},
   "source": [
    "# Parameters and model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4d402bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    22\n",
       "negative    18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = imdb.sample(n = 40)\n",
    "test['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ca9a69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "# Hyperparameters of the model\n",
    "vocab_size = 3000 # choose based on statistics\n",
    "oov_tok = ''\n",
    "embedding_dim = 100\n",
    "max_length = 200 # choose based on statistics, for example 150 to 200\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95dec5",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bd510ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 200, 100)          300000    \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirect  (None, 128)               84480     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 387601 (1.48 MB)\n",
      "Trainable params: 387601 (1.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_sentences = small_db['review'].values\n",
    "train_labels = encoder.fit_transform(small_db['sentiment'].values)\n",
    "\n",
    "# tokenize sentences\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fd92ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 [==============================] - 13s 765ms/step - loss: 0.6937 - accuracy: 0.5444 - val_loss: 0.6964 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 474ms/step - loss: 0.6796 - accuracy: 0.5500 - val_loss: 0.7030 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 3s 505ms/step - loss: 0.6552 - accuracy: 0.5778 - val_loss: 0.7140 - val_accuracy: 0.4500\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 524ms/step - loss: 0.5861 - accuracy: 0.7333 - val_loss: 1.0779 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 4s 645ms/step - loss: 0.5228 - accuracy: 0.8389 - val_loss: 0.7184 - val_accuracy: 0.3000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bea1e6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Accuracy of prediction on test set :  0.45\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(test_padded)\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "pred_labels = []\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1005d7",
   "metadata": {},
   "source": [
    "## test on whole paragraph paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "649db10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 200, 100)          300000    \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirect  (None, 128)               84480     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 387601 (1.48 MB)\n",
      "Trainable params: 387601 (1.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_sentences = para_db['review'].values\n",
    "train_labels = encoder.fit_transform(para_db['sentiment'].values)\n",
    "\n",
    "# tokenize sentences\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "da44d6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14/14 [==============================] - 13s 472ms/step - loss: 0.6876 - accuracy: 0.5613 - val_loss: 0.6186 - val_accuracy: 0.7917\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 5s 346ms/step - loss: 0.6724 - accuracy: 0.5825 - val_loss: 0.6055 - val_accuracy: 0.7917\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 4s 317ms/step - loss: 0.5890 - accuracy: 0.7288 - val_loss: 0.6142 - val_accuracy: 0.8125\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 4s 310ms/step - loss: 0.3851 - accuracy: 0.9151 - val_loss: 0.3448 - val_accuracy: 0.8750\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 4s 315ms/step - loss: 0.0851 - accuracy: 0.9811 - val_loss: 0.3157 - val_accuracy: 0.8542\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bfb6566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Accuracy of prediction on test set :  0.65\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(test_padded)\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "pred_labels = []\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(test_labels,pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
