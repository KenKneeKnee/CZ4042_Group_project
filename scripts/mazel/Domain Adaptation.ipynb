{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pytreebank\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = os.path.join('..', '..', 'data', 'IMDB Dataset.csv')\n",
    "\n",
    "if os.path.exists(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(df.head())\n",
    "else:           \n",
    "    print(\"CSV file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {'positive': 1, 'negative': 0}\n",
    "df['sentiment_number'] = df['sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "       sentiment_number  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     0  \n",
       "4                     1  \n",
       "...                 ...  \n",
       "49995                 1  \n",
       "49996                 0  \n",
       "49997                 0  \n",
       "49998                 0  \n",
       "49999                 0  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(strip_html)\n",
    "df['review']=df['review'].apply(preprocess_text)\n",
    "df['review']=df['review'].apply(remove_punctuation)\n",
    "df['review']=df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Shape: (35000, 3)\n",
      "Test Data:\n",
      "Shape: (15000, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes of the train and test DataFrames\n",
    "print(\"Train Data:\")\n",
    "print(\"Shape:\", train_df.shape)\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print(\"Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['review'], train_df['sentiment_number']))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_df['review'], test_df['sentiment_number']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 128)               124642688 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                2064      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124644769 (475.48 MB)\n",
      "Trainable params: 124644769 (475.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "69/69 [==============================] - 27s 381ms/step - loss: 0.4718 - accuracy: 0.7321 - val_loss: 0.3063 - val_accuracy: 0.8720\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 26s 381ms/step - loss: 0.2220 - accuracy: 0.9105 - val_loss: 0.2480 - val_accuracy: 0.8927\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 26s 377ms/step - loss: 0.1380 - accuracy: 0.9498 - val_loss: 0.2544 - val_accuracy: 0.8919\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 26s 380ms/step - loss: 0.0862 - accuracy: 0.9734 - val_loss: 0.2822 - val_accuracy: 0.8877\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 26s 380ms/step - loss: 0.0517 - accuracy: 0.9871 - val_loss: 0.3217 - val_accuracy: 0.8861\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 26s 377ms/step - loss: 0.0296 - accuracy: 0.9945 - val_loss: 0.3639 - val_accuracy: 0.8805\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 26s 383ms/step - loss: 0.0170 - accuracy: 0.9980 - val_loss: 0.4059 - val_accuracy: 0.8789\n"
     ]
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "\n",
    "    train_dataset.shuffle(10000).batch(512),\n",
    "    epochs=100,\n",
    "    validation_data=test_dataset.batch(512),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  (None, 250)               252343750 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                4016      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252347783 (962.63 MB)\n",
      "Trainable params: 252347783 (962.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mazeltan/Desktop/CZ4042_Group_project/env/lib/python3.11/site-packages/keras/src/backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 57s 799ms/step - loss: 0.4458 - accuracy: 0.8186 - val_loss: 0.2898 - val_accuracy: 0.8849\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 78s 1s/step - loss: 0.2146 - accuracy: 0.9190 - val_loss: 0.2527 - val_accuracy: 0.8971\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 77s 1s/step - loss: 0.1388 - accuracy: 0.9523 - val_loss: 0.2701 - val_accuracy: 0.8911\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 82s 1s/step - loss: 0.0925 - accuracy: 0.9721 - val_loss: 0.2996 - val_accuracy: 0.8873\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 74s 1s/step - loss: 0.0610 - accuracy: 0.9847 - val_loss: 0.3460 - val_accuracy: 0.8796\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 71s 1s/step - loss: 0.0384 - accuracy: 0.9926 - val_loss: 0.3929 - val_accuracy: 0.8759\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 72s 1s/step - loss: 0.0242 - accuracy: 0.9964 - val_loss: 0.4401 - val_accuracy: 0.8743\n"
     ]
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\"\n",
    "\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "model= tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "metrics=['accuracy'])\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_df['review'], train_df['sentiment_number']))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_df['review'], test_df['sentiment_number']))\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "\n",
    "    train_dataset.shuffle(10000).batch(512),\n",
    "    epochs=100,\n",
    "    validation_data=test_dataset.batch(512),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STANFORD SENTIMENT TREEBANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pytreebank.load_sst(\"data/SST2-Data/SST2-Data/trainDevTestTrees_PTB/trees/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(\"data/sst_{}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with <_io.TextIOWrapper name='data/sst_train.txt' mode='w' encoding='UTF-8'>\n",
      "done with <_io.TextIOWrapper name='data/sst_test.txt' mode='w' encoding='UTF-8'>\n",
      "done with <_io.TextIOWrapper name='data/sst_dev.txt' mode='w' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "for cat in ['train','test','dev']:\n",
    "    with open(out_path.format(cat),\"w\") as file:\n",
    "        for item in data[cat]:\n",
    "            file.write(\"__label__{}\\t{}\\n\".format(\n",
    "                item.to_labeled_lines()[0][0] +1,\n",
    "                item.to_labeled_lines()[0][1]\n",
    "            ))\n",
    "    \n",
    "    print(\"done with {}\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = pd.read_csv(\"data/sst_train.txt\",sep=\"\\t\",header=None,names=['label','text'])\n",
    "train_df2['label'] = train_df2['label'].str.replace(\"__label__\",\"\")\n",
    "# df_train['label'] = df_train['label'].astype(int).astype(\"category\")\n",
    "train_df2['label'] = (train_df2['label'].astype(int) - 1).astype('category')\n",
    "\n",
    "test_df2 = pd.read_csv(\"data/sst_test.txt\",sep=\"\\t\",header=None,names=['label','text'])\n",
    "test_df2['label'] = test_df2['label'].str.replace(\"__label__\",\"\")\n",
    "# df_test['label'] = df_test['label'].astype(int).astype(\"category\")\n",
    "test_df2['label'] = (test_df2['label'].astype(int) - 1).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0s/6y_9xhvn7tx64mnfljyvm01m0000gn/T/ipykernel_81496/584409625.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "train_df2['text'] = train_df2['text'].apply(strip_html)\n",
    "train_df2['text'] = train_df2['text'].apply(preprocess_text)\n",
    "train_df2['text'] = train_df2['text'].apply(remove_punctuation)\n",
    "train_df2['text'] = train_df2['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0s/6y_9xhvn7tx64mnfljyvm01m0000gn/T/ipykernel_81496/584409625.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "test_df2['text'] = test_df2['text'].apply(strip_html)\n",
    "test_df2['text'] = test_df2['text'].apply(preprocess_text)\n",
    "test_df2['text'] = test_df2['text'].apply(remove_punctuation)\n",
    "test_df2['text'] = test_df2['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df2['text'], train_df2['label']))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_df2['text'], test_df2['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/267 [..............................] - ETA: 48s - loss: 1.6192 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mazeltan/Desktop/CZ4042_Group_project/env/lib/python3.11/site-packages/keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/267 [==============================] - 1s 1ms/step - loss: 1.4900 - accuracy: 0.3758 - val_loss: 1.7698 - val_accuracy: 0.2864\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 0s 839us/step - loss: 1.4555 - accuracy: 0.3672 - val_loss: 1.6508 - val_accuracy: 0.2937\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 0s 830us/step - loss: 1.3848 - accuracy: 0.3971 - val_loss: 1.6183 - val_accuracy: 0.3027\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 0s 826us/step - loss: 1.3482 - accuracy: 0.4101 - val_loss: 1.5934 - val_accuracy: 0.3190\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 0s 835us/step - loss: 1.3262 - accuracy: 0.4222 - val_loss: 1.5793 - val_accuracy: 0.3267\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 0s 910us/step - loss: 1.3117 - accuracy: 0.4292 - val_loss: 1.5704 - val_accuracy: 0.3353\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 0s 833us/step - loss: 1.3022 - accuracy: 0.4359 - val_loss: 1.5621 - val_accuracy: 0.3439\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 0s 835us/step - loss: 1.2955 - accuracy: 0.4389 - val_loss: 1.5543 - val_accuracy: 0.3448\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 0s 866us/step - loss: 1.2914 - accuracy: 0.4422 - val_loss: 1.5461 - val_accuracy: 0.3511\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 0s 849us/step - loss: 1.2878 - accuracy: 0.4453 - val_loss: 1.5417 - val_accuracy: 0.3498\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 0s 851us/step - loss: 1.2856 - accuracy: 0.4469 - val_loss: 1.5364 - val_accuracy: 0.3520\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 0s 678us/step - loss: 1.2817 - accuracy: 0.4498 - val_loss: 1.5403 - val_accuracy: 0.3534\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 0s 647us/step - loss: 1.2779 - accuracy: 0.4499 - val_loss: 1.5520 - val_accuracy: 0.3543\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 0s 661us/step - loss: 1.2740 - accuracy: 0.4527 - val_loss: 1.5438 - val_accuracy: 0.3529\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 0s 653us/step - loss: 1.2709 - accuracy: 0.4541 - val_loss: 1.5440 - val_accuracy: 0.3529\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 0s 919us/step - loss: 1.2681 - accuracy: 0.4553 - val_loss: 1.5368 - val_accuracy: 0.3520\n"
     ]
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=False)  # You can set trainable to True or False based on your needs\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Batch the datasets\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,      \n",
    "    epochs=50,                                 \n",
    "    validation_data=test_dataset, \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/267 [..............................] - ETA: 47s - loss: 1.6216 - accuracy: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mazeltan/Desktop/CZ4042_Group_project/env/lib/python3.11/site-packages/keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/267 [==============================] - 1s 2ms/step - loss: 1.4737 - accuracy: 0.3923 - val_loss: 1.5824 - val_accuracy: 0.2864\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 0s 636us/step - loss: 1.4771 - accuracy: 0.2973 - val_loss: 1.6254 - val_accuracy: 0.2869\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 0s 587us/step - loss: 1.4134 - accuracy: 0.3865 - val_loss: 1.7825 - val_accuracy: 0.2900\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 0s 592us/step - loss: 1.3947 - accuracy: 0.4106 - val_loss: 1.7264 - val_accuracy: 0.2941\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 0s 582us/step - loss: 1.3771 - accuracy: 0.4096 - val_loss: 1.7463 - val_accuracy: 0.2964\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 0s 1ms/step - loss: 1.3547 - accuracy: 0.4204 - val_loss: 1.7253 - val_accuracy: 0.2968\n"
     ]
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=False)  # You can set trainable to True or False based on your needs\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,      \n",
    "    epochs=50,                                 \n",
    "    validation_data=test_dataset, \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
